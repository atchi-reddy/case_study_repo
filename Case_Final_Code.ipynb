{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b342d9-a1e2-412b-b6ed-b416e5e32567",
   "metadata": {},
   "source": [
    "# Python Anomalous System Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebd56ef1-07c5-45ce-a36a-408b849411d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (1.37.34)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.34 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from boto3) (1.37.34)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from boto3) (0.11.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from botocore<1.38.0,>=1.37.34->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from botocore<1.38.0,>=1.37.34->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\reddy\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.34->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "## Install & Import required packages\n",
    "!pip install boto3\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "from io import StringIO\n",
    "# from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a3485c5-7020-4d09-ae43-483fbf969779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializaing the S3 Bucket Service\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO \n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466946cf-6b1b-45c4-b4b2-67f9f04c235d",
   "metadata": {},
   "source": [
    "# Generating the Clean Stock data using random walk with drift for simulating realistic stock behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27fcac51-97e2-457c-b49e-8ec5883b6f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows with first 5 stocks:\n",
      "            ST0001  ST0002  ST0003  ST0004  ST0005\n",
      "2020-01-01  193.52  475.85  368.68  303.34   86.45\n",
      "2020-01-02  193.77  473.77  376.29  308.03   83.42\n",
      "2020-01-03  194.12  471.61  379.94  307.19   82.28\n",
      "2020-01-06  193.24  474.64  385.50  307.94   84.73\n",
      "2020-01-07  193.08  470.48  383.37  308.49   83.52\n",
      "\n",
      "Last 5 rows with last 5 stocks:\n",
      "            ST0096  ST0097  ST0098  ST0099  ST0100\n",
      "2022-09-20  215.76  569.02  311.78   36.20   75.77\n",
      "2022-09-21  215.96  566.47  310.75   36.11   74.43\n",
      "2022-09-22  211.76  577.29  306.07   35.62   74.83\n",
      "2022-09-23  213.64  582.33  310.10   35.40   74.60\n",
      "2022-09-26  214.46  576.71  307.61   34.70   74.91\n"
     ]
    }
   ],
   "source": [
    "# Here we can generate similar dataset for 40000 dates for 1000 stocks but due to scalability I have taken 1000 days and 100 stocks\n",
    "# The original dataset for 40000 dates with 1000 stocks can be generated \n",
    "\n",
    "def generate_clean_stock_data(num_days=1000, num_stocks=100, start_date=None, seed=42): \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    if start_date is None:\n",
    "        start_date = datetime.date(2020, 1, 1)\n",
    "    dates = [start_date + datetime.timedelta(days=i) for i in range(num_days)]\n",
    "    dates = [date for date in dates if date.weekday() < 5]      # Filter out weekends\n",
    "    tickers = [f\"ST{i:04d}\" for i in range(1, num_stocks + 1)]  # Create stock tickers in format ST0001, ST0002, etc.\n",
    "    initial_prices = np.random.uniform(10, 500, num_stocks)     # Generate initial prices (between $10 and $500)\n",
    "    df = pd.DataFrame(index=dates, columns=tickers)\n",
    "    df.iloc[0] = initial_prices\n",
    "    \n",
    "    # Generate daily returns using a random walk with drift\n",
    "    annual_drift = np.random.uniform(0.1, 0.25, num_stocks)  # 10% to 25% annual return # 10% to +25% returns \n",
    "    annual_volatility = np.random.uniform(0.1, 0.4, num_stocks)  # 10% to 40% annual volatility # 40 % to +40% (Analysis and need to add expereinces)\n",
    "    \n",
    "    # Convert annual parameters to daily\n",
    "    trading_days_per_year = 252\n",
    "    daily_drift = annual_drift / trading_days_per_year\n",
    "    daily_volatility = annual_volatility / np.sqrt(trading_days_per_year)\n",
    "    \n",
    "    # Generate price series & daily returns as random normal with mean=drift and std=volatility\n",
    "    for i in range(1, len(dates)):\n",
    "        daily_returns = np.random.normal(daily_drift,daily_volatility,num_stocks)\n",
    "        df.iloc[i] = df.iloc[i-1] * (1 + daily_returns) # Apply returns to previous day's prices\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: float(f\"{x:.2f}\") if not pd.isna(x) else x) # keep the decimal to 2 points\n",
    "    return df\n",
    "\n",
    "# store the data in to clean_dataframe\n",
    "clean_data = generate_clean_stock_data(num_days=1000, num_stocks=100)\n",
    "\n",
    "# Display the first 5 and last 5 rows to verify\n",
    "print(\"\\nFirst 5 rows with first 5 stocks:\")\n",
    "print(clean_data.iloc[:5, :5])\n",
    "print(\"\\nLast 5 rows with last 5 stocks:\")\n",
    "print(clean_data.iloc[-5:, -5:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69a2dd-6925-4c86-a0f3-b69c2a79dabb",
   "metadata": {},
   "source": [
    "# Generating the Anomalous Stock data and Introducing anomalies to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c12990c-2d00-45ed-becd-15a94e3dc834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing 3570 anomalies into the dataset...\n",
      "\n",
      "Anomaly distribution by type:\n",
      "anomaly_type\n",
      "price_drop         904\n",
      "price_spike        876\n",
      "duplicate_value    559\n",
      "missing_value      514\n",
      "zero_price         391\n",
      "missing_ticker     166\n",
      "negative_price     160\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample comparison for date 2020-05-20:\n",
      "Clean data:\n",
      "ST0001    237.51\n",
      "ST0002    503.06\n",
      "ST0003    435.62\n",
      "ST0004    371.12\n",
      "ST0005    103.53\n",
      "Name: 2020-05-20, dtype: float64\n",
      "\n",
      "Anomalous data:\n",
      "ST0001    241.21\n",
      "ST0002    503.06\n",
      "ST0003    435.62\n",
      "ST0004    371.12\n",
      "ST0005    103.53\n",
      "Name: 2020-05-20, dtype: float64\n",
      "\n",
      "Total values in dataset: 71400\n",
      "Changed values: 3492 (4.89% of total)\n",
      "Null values introduced: 673\n",
      "\n",
      "Examples of each anomaly type:\n",
      "\n",
      "DUPLICATE_VALUE:\n",
      "Date: 2020-05-22, Ticker: ST0024\n",
      "Original value: 207.76, New value: 200.82\n",
      "\n",
      "MISSING_VALUE:\n",
      "Date: 2021-09-01, Ticker: ST0079\n",
      "Original value: 189.11, New value: nan\n",
      "\n",
      "NEGATIVE_PRICE:\n",
      "Date: 2021-01-13, Ticker: ST0022\n",
      "Original value: 81.09, New value: -81.09\n",
      "\n",
      "PRICE_SPIKE:\n",
      "Date: 2022-05-10, Ticker: ST0059\n",
      "Original value: 30.69, New value: 81.95096581836674\n",
      "\n",
      "PRICE_DROP:\n",
      "Date: 2020-06-18, Ticker: ST0087\n",
      "Original value: 398.93, New value: 180.56133355246314\n",
      "\n",
      "ZERO_PRICE:\n",
      "Date: 2020-07-01, Ticker: ST0075\n",
      "Original value: 363.46, New value: 0.0\n",
      "\n",
      "MISSING_TICKER:\n",
      "Date: 2022-03-02, Ticker: ST0055\n",
      "Original value: 564.6, New value: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def introduce_anomalies(clean_df, anomaly_percentage=0.05, seed=42): # 10% for anomalies \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Calculate number of anomalies to introduce\n",
    "    anomalous_df = clean_df.copy()\n",
    "    num_days = len(clean_df)\n",
    "    num_stocks = len(clean_df.columns)\n",
    "    num_anomalies = int(num_days * num_stocks * anomaly_percentage)\n",
    "    print(f\"Introducing {num_anomalies} anomalies into the dataset...\")\n",
    "    \n",
    "    # Create anomaly types and their relative frequencies\n",
    "    anomaly_types = [\n",
    "        \"price_spike\",         # Sudden large increase in price\n",
    "        \"price_drop\",          # Sudden large decrease in price\n",
    "        \"missing_value\",       # Null value\n",
    "        \"duplicate_value\",     # Same price repeated\n",
    "        \"zero_price\",          # Price set to zero\n",
    "        \"negative_price\",      # Negative price (impossible in reality)\n",
    "        \"missing_ticker\"       # Missing ticker for a day\n",
    "    ]\n",
    "    \n",
    "    anomaly_weights = [0.25, 0.25, 0.15, 0.15, 0.1, 0.05, 0.05]\n",
    "    \n",
    "    # Randomly select days and stocks for anomalies\n",
    "    anomaly_days = np.random.choice(range(num_days), num_anomalies, replace=True)\n",
    "    anomaly_stocks = np.random.choice(clean_df.columns, num_anomalies, replace=True)\n",
    "    anomaly_types_selected = np.random.choice(anomaly_types, num_anomalies, replace=True, p=anomaly_weights)\n",
    "    \n",
    "    # Introduce anomalies\n",
    "    anomaly_log = []\n",
    "    \n",
    "    for i in range(num_anomalies):\n",
    "        day_idx = anomaly_days[i]\n",
    "        stock = anomaly_stocks[i]\n",
    "        anomaly_type = anomaly_types_selected[i]\n",
    "        original_value = anomalous_df.iloc[day_idx][stock]\n",
    "        \n",
    "        # Apply the anomaly based on its type\n",
    "        if anomaly_type == \"price_spike\":\n",
    "            # Multiply price by 1.5 to 6x\n",
    "            factor = np.random.uniform(1.5, 6.0)\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = original_value * factor\n",
    "            new_value = anomalous_df.iloc[day_idx][stock]\n",
    "            \n",
    "        elif anomaly_type == \"price_drop\":\n",
    "            # Multiply price by 0.1 to 0.7\n",
    "            factor = np.random.uniform(0.1, 0.7)\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = original_value * factor\n",
    "            new_value = anomalous_df.iloc[day_idx][stock]\n",
    "            \n",
    "        elif anomaly_type == \"missing_value\":\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = np.nan\n",
    "            new_value = np.nan\n",
    "            \n",
    "        elif anomaly_type == \"duplicate_value\":\n",
    "            if day_idx > 0:\n",
    "                anomalous_df.loc[anomalous_df.index[day_idx], stock] = anomalous_df.iloc[day_idx-1][stock]\n",
    "                new_value = anomalous_df.iloc[day_idx][stock]\n",
    "            else:\n",
    "                new_value = original_value\n",
    "                \n",
    "        elif anomaly_type == \"zero_price\":\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = 0.0\n",
    "            new_value = 0.0\n",
    "            \n",
    "        elif anomaly_type == \"negative_price\":\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = -original_value\n",
    "            new_value = anomalous_df.iloc[day_idx][stock]\n",
    "            \n",
    "        elif anomaly_type == \"missing_ticker\":\n",
    "            anomalous_df.loc[anomalous_df.index[day_idx], stock] = np.nan\n",
    "            new_value = np.nan\n",
    "        \n",
    "        # Log the anomaly\n",
    "        anomaly_log.append({ 'date': anomalous_df.index[day_idx], 'ticker': stock,'anomaly_type': anomaly_type,\n",
    "            'original_value': original_value, 'new_value': new_value })\n",
    "    \n",
    "    # Create a dataframe with the anomaly log\n",
    "    anomaly_report = pd.DataFrame(anomaly_log)\n",
    "    for col in anomalous_df.columns:\n",
    "        anomalous_df[col] = anomalous_df[col].apply(lambda x: float(f\"{x:.2f}\") if not pd.isna(x) else x)\n",
    "    \n",
    "    \n",
    "    return anomalous_df, anomaly_report\n",
    "\n",
    "# Introduce anomalies (5% of all data points will contain anomalies)\n",
    "anomalous_data, anomaly_report = introduce_anomalies(clean_data, anomaly_percentage=0.05)\n",
    "\n",
    "# Display summary of the anomalies introduced\n",
    "print(\"\\nAnomaly distribution by type:\")\n",
    "print(anomaly_report['anomaly_type'].value_counts())\n",
    "\n",
    "# Compare sample of clean vs anomalous data\n",
    "sample_date = clean_data.index[100]  \n",
    "sample_stocks = clean_data.columns[:5] \n",
    "\n",
    "print(f\"\\nSample comparison for date {sample_date}:\")\n",
    "print(\"Clean data:\")\n",
    "print(clean_data.loc[sample_date, sample_stocks])\n",
    "print(\"\\nAnomalous data:\")\n",
    "print(anomalous_data.loc[sample_date, sample_stocks])\n",
    "\n",
    "# Calculate how many values were changed\n",
    "total_values = anomalous_data.size\n",
    "changed_values = np.sum(clean_data.values != anomalous_data.values)\n",
    "null_values = anomalous_data.isna().sum().sum()\n",
    "\n",
    "print(f\"\\nTotal values in dataset: {total_values}\")\n",
    "print(f\"Changed values: {changed_values} ({changed_values/total_values:.2%} of total)\")\n",
    "print(f\"Null values introduced: {null_values}\")\n",
    "\n",
    "# Display examples of different types of anomalies\n",
    "print(\"\\nExamples of each anomaly type:\")\n",
    "for anomaly_type in anomaly_report['anomaly_type'].unique():\n",
    "    example = anomaly_report[anomaly_report['anomaly_type'] == anomaly_type].iloc[0]\n",
    "    print(f\"\\n{anomaly_type.upper()}:\")\n",
    "    print(f\"Date: {example['date']}, Ticker: {example['ticker']}\")\n",
    "    print(f\"Original value: {example['original_value']}, New value: {example['new_value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3687d1-0372-4739-ba46-19451e84a966",
   "metadata": {},
   "source": [
    "# Anomaly Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1a4d67c-3db6-46ed-beb7-650fc88afe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping threshold up and down and moving average of 7 days for measurement metrics\n",
    "def detect_anomalies_against_history(historical_df, new_df, threshold_up=0.2, threshold_down=-0.2, window=7):\n",
    "    \n",
    "    historical_df.index = pd.to_datetime(historical_df.index)\n",
    "    new_df.index = pd.to_datetime(new_df.index)\n",
    "\n",
    "    if historical_df.shape[0] < window:\n",
    "        raise ValueError(\"Not enough historical data to compute moving average.\")\n",
    "    \n",
    "    current_date = new_df.index[0]\n",
    "    today_prices = new_df.iloc[0]\n",
    "    moving_avg = historical_df.iloc[-window:].mean()\n",
    "    expected_tickers = list(historical_df.columns)\n",
    "    incoming_tickers = list(new_df.columns)\n",
    "\n",
    "    anomalies = {}\n",
    "\n",
    "    # Check for the anomalies in the data\n",
    "    for i, col in enumerate(incoming_tickers):\n",
    "        if pd.isna(col) or str(col).strip() == \"\":\n",
    "            anomalies[f\"column_{i}\"] = \"Unnamed/Blank Ticker Column\"\n",
    "\n",
    "    for i, expected_ticker in enumerate(expected_tickers):\n",
    "        if expected_ticker not in incoming_tickers:\n",
    "            anomalies[f\"column_{i}\"] = f\"Missing Ticker at Position {i}\"\n",
    "\n",
    "    duplicate_tickers = new_df.columns[new_df.columns.duplicated()].tolist()\n",
    "    for dup in duplicate_tickers:\n",
    "        anomalies[dup] = \"Duplicate Ticker Name\"\n",
    "\n",
    "    if current_date in historical_df.index:\n",
    "        anomalies['__DUPLICATE_DATE__'] = f\"Duplicate Date Entry ({current_date.date()})\"\n",
    "\n",
    "    for ticker in incoming_tickers:\n",
    "        if ticker not in expected_tickers or pd.isna(ticker) or str(ticker).strip() == \"\":\n",
    "            continue  # skip unexpected or unnamed tickers\n",
    "\n",
    "        today_price = today_prices[ticker]\n",
    "        avg_price = moving_avg.get(ticker, np.nan)\n",
    "\n",
    "        if pd.isna(today_price):\n",
    "            anomalies[ticker] = \"Missing/Null Price (value=None)\"\n",
    "            continue\n",
    "\n",
    "        if today_price == 0:\n",
    "            anomalies[ticker] = \"Zero Price (value=0.00)\"\n",
    "            continue\n",
    "\n",
    "        if today_price < 0:\n",
    "            anomalies[ticker] = f\"Negative Price (value={today_price:.2f})\"\n",
    "            continue\n",
    "\n",
    "        if pd.isna(avg_price) or avg_price == 0:\n",
    "            anomalies[ticker] = \"Invalid Moving Average\"\n",
    "            continue\n",
    "\n",
    "        pct_change = (today_price - avg_price) / avg_price\n",
    "        if pct_change > threshold_up:\n",
    "            anomalies[ticker] = f\"Price Increase > {threshold_up*100:.0f}% ({pct_change*100:.2f}%)\"\n",
    "        elif pct_change < threshold_down:\n",
    "            anomalies[ticker] = f\"Price Decrease < {abs(threshold_down*100):.0f}% ({pct_change*100:.2f}%)\"\n",
    "\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d4dcb1c-5149-41e4-a65f-6358f8215d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected 14 anomalies on 2022-09-26:\n",
      "\n",
      "column_99: Unnamed/Blank Ticker Column\n",
      "column_5: Missing Ticker at Position 5\n",
      "ST0002: Price Decrease < 20% (-23.23%)\n",
      "ST0005: Price Decrease < 20% (-28.53%)\n",
      "ST0009: Price Increase > 20% (120.54%)\n",
      "ST0018: Zero Price (value=0.00)\n",
      "ST0021: Price Decrease < 20% (-39.75%)\n",
      "ST0055: Missing/Null Price (value=None)\n",
      "ST0059: Price Increase > 20% (44.75%)\n",
      "ST0070: Price Increase > 20% (40.64%)\n",
      "ST0076: Price Increase > 20% (20.95%)\n",
      "ST0080: Price Decrease < 20% (-36.44%)\n",
      "ST0081: Price Increase > 20% (207.37%)\n",
      "ST0096: Price Increase > 20% (41.78%)\n"
     ]
    }
   ],
   "source": [
    "# Keep the last record as new record for new_df\n",
    "historical_df = anomalous_data.iloc[:-1]\n",
    "new_df = anomalous_data.iloc[[-1]]\n",
    "new_df = new_df.drop(columns=[\"ST0006\"])  # simulate missing ticker ST0006\n",
    "new_df.columns = list(new_df.columns)\n",
    "new_df[\"\"] = 123.45                       # simulate a blank column for new_df\n",
    "\n",
    "# Run anomaly detection\n",
    "anomalies = detect_anomalies_against_history(historical_df, new_df)\n",
    "print(f\"\\nDetected {len(anomalies)} anomalies on {new_df.index[0].date()}:\\n\")\n",
    "for ticker, issue in anomalies.items():\n",
    "    print(f\"{ticker}: {issue}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1064fb73-2180-44ca-a552-0bd89b7bb5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'DSTXVRAH3552W9N3',\n",
       "  'HostId': 'Y2eBTp9ny5cAPotadvFLBm4iD35542FJ1mjAKj2N42jXi5gpV0Ok1l2Yo2wQvStv68QVSG8zIfA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'Y2eBTp9ny5cAPotadvFLBm4iD35542FJ1mjAKj2N42jXi5gpV0Ok1l2Yo2wQvStv68QVSG8zIfA=',\n",
       "   'x-amz-request-id': 'DSTXVRAH3552W9N3',\n",
       "   'date': 'Tue, 15 Apr 2025 08:20:52 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"e7f9046769e552f0f670315d801f0194\"',\n",
       "   'x-amz-checksum-crc32': '/2aA1A==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"e7f9046769e552f0f670315d801f0194\"',\n",
       " 'ChecksumCRC32': '/2aA1A==',\n",
       " 'ChecksumType': 'FULL_OBJECT',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Storing the results in the S3 Bucket\n",
    "df = pd.DataFrame([{'Ticker/Key': k, 'Issue': v} for k, v in anomalies.items()])\n",
    "csv_buffer = StringIO()\n",
    "df.to_csv(csv_buffer, index=False)\n",
    "s3_key = f\"anomalies/anomaly_report_1.csv\"\n",
    "\n",
    "# 5. Upload to S3\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_object(Bucket='atchi-reddy-s3-bucket', Key=s3_key, Body=csv_buffer.getvalue() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a58b3468-b3f6-464f-b0a3-519ae3daf7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket='atchi-reddy-s3-bucket', Prefix='anomalies/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c121b07-0673-47f2-847c-8272be5fbf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker/Key</th>\n",
       "      <th>Issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>column_99</td>\n",
       "      <td>Unnamed/Blank Ticker Column</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>column_5</td>\n",
       "      <td>Missing Ticker at Position 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST0002</td>\n",
       "      <td>Price Decrease &lt; 20% (-23.23%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST0005</td>\n",
       "      <td>Price Decrease &lt; 20% (-28.53%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST0009</td>\n",
       "      <td>Price Increase &gt; 20% (120.54%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker/Key                           Issue\n",
       "0  column_99     Unnamed/Blank Ticker Column\n",
       "1   column_5    Missing Ticker at Position 5\n",
       "2     ST0002  Price Decrease < 20% (-23.23%)\n",
       "3     ST0005  Price Decrease < 20% (-28.53%)\n",
       "4     ST0009  Price Increase > 20% (120.54%)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking the file which stored in S3 bucket \n",
    "csv_obj = s3.get_object(Bucket=\"atchi-reddy-s3-bucket\", Key=\"anomalies/anomaly_report_1.csv\")\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(csv_string))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70704f6f-becc-45b6-91dd-948357a18bd9",
   "metadata": {},
   "source": [
    "# Mail Delivery System for Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d70eda4-abb2-410d-ae20-ff26e2399f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to send email: (535, b'5.7.8 Username and Password not accepted. For more information, go to\\n5.7.8  https://support.google.com/mail/?p=BadCredentials 6a1803df08f44-6f0dea10825sm98041446d6.116 - gsmtp')\n"
     ]
    }
   ],
   "source": [
    "# Haven't attached the sensitive details \n",
    "\n",
    "def send_email_alert(sender_email, app_password, recipient_email, subject, body):\n",
    "    msg = EmailMessage()\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "    msg.set_content(body)\n",
    "    \n",
    "    try:\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
    "            smtp.login(sender_email, app_password)\n",
    "            smtp.send_message(msg)\n",
    "            print(\"Email alert sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email: {e}\")\n",
    "\n",
    "def format_anomalies_for_email(anomalies, date):\n",
    "    body_lines = [f\"ðŸ“… Detected {len(anomalies)} anomalies on {date.date()}:\\n\"]\n",
    "    for key, issue in anomalies.items():\n",
    "        body_lines.append(f\"{key}: {issue}\")\n",
    "    return \"\\n\".join(body_lines)\n",
    "\n",
    "email_body = format_anomalies_for_email(anomalies, new_df.index[0])\n",
    "\n",
    "send_email_alert(\n",
    "    sender_email=\"your_email@gmail.com\",\n",
    "    app_password=\"your_app_password_here\",\n",
    "    recipient_email=\"recipient_email@example.com\",\n",
    "    subject=\" Stock Anomaly Alert - \" + str(new_df.index[0].date()),\n",
    "    body=email_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41dbe1d-3447-455f-970d-9eba28548cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
